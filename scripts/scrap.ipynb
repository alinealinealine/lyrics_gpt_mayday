{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to scrap lyrics from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very old method: get the element and extract the relevant text. Because the website dynamic action making it hard to extract all links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open and read the file\n",
    "with open('element_all_song.txt', 'r') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Get the divs for the song titles and the a tags for the song URLs\n",
    "titles = soup.select('div.mini_card-title')\n",
    "urls = soup.select('mini-song-card a')\n",
    "\n",
    "# Extract the URLs from each a tag\n",
    "urls = [url['href'] for url in urls]\n",
    "\n",
    "# Extract the Chinese and English title from each title div\n",
    "chinese_titles = []\n",
    "english_titles = []\n",
    "\n",
    "for title in titles:\n",
    "    parts = title.text.split(' (')\n",
    "    chinese_titles.append(parts[0])\n",
    "    if len(parts) > 1:\n",
    "        english_titles.append(parts[1].replace(')', ''))\n",
    "    else:\n",
    "        english_titles.append(None)\n",
    "\n",
    "# Print the URLs and titles\n",
    "for url, chi_title, eng_title in zip(urls, chinese_titles, english_titles):\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Chinese Title: {chi_title}\")\n",
    "    print(f\"English Title: {eng_title}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'URL': urls,\n",
    "    'Title': chinese_titles,\n",
    "    'English Title': english_titles\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each song page, extract the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm  # This library is for progress bars. It's optional.\n",
    "\n",
    "# Initialize an empty list to store the lyrics.\n",
    "lyrics_list = []\n",
    "\n",
    "# Loop over each URL in the DataFrame.\n",
    "for url in tqdm(df['URL']):\n",
    "    # Send a GET request to the URL.\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content of the page with BeautifulSoup.\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the lyrics div and extract the text.\n",
    "    lyrics_div = soup.find('div', attrs={\"data-lyrics-container\": \"true\"})\n",
    "    lyrics = lyrics_div.get_text(separator='\\n')\n",
    "    \n",
    "    # Append the lyrics to the lyrics_list.\n",
    "    lyrics_list.append(lyrics)\n",
    "\n",
    "# Add the lyrics_list as a new column in the DataFrame.\n",
    "df['Lyrics'] = lyrics_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to JSON file to match the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'URL': urls,\n",
    "#     'Title': chinese_titles,\n",
    "#     'English Title': english_titles,\n",
    "#     'Lyrics': lyrics_list\n",
    "# })\n",
    "\n",
    "def get_chunks(content, url, title):\n",
    "    content_length = len(content)\n",
    "    content_tokens = len(list(jieba.cut(content, cut_all=False)))\n",
    "    chunk = {\n",
    "        \"essay_title\": title,\n",
    "        \"essay_url\": url,\n",
    "        \"essay_date\": \"NA\",\n",
    "        \"essay_thanks\": \"NA\",\n",
    "        \"content\": content,\n",
    "        \"content_length\": content_length,\n",
    "        \"content_tokens\": content_tokens,\n",
    "        \"embedding\": []\n",
    "    }\n",
    "    return [chunk]  # As chunks is an array, wrap the chunk object in a list\n",
    "\n",
    "data = {\n",
    "    \"current_date\": \"2023-03-01\",\n",
    "    \"author\": \"Your name here\",\n",
    "    \"url\": \"Your URL here\",\n",
    "    \"length\": df['Lyrics'].apply(len).sum().item(),\n",
    "    \"tokens\": df['Lyrics'].apply(lambda x: len(list(jieba.cut(x, cut_all=False)))).sum().item(),\n",
    "    \"essays\": []\n",
    "}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    essay = {\n",
    "        \"title\": row['Title'],\n",
    "        \"url\": row['URL'],\n",
    "        \"date\": \"NA\",\n",
    "        \"thanks\": \"NA\",\n",
    "        \"content\": row['Lyrics'],\n",
    "        \"length\": len(row['Lyrics']),\n",
    "        \"tokens\": len(list(jieba.cut(row['Lyrics'], cut_all=False))),\n",
    "        \"chunks\": get_chunks(row['Lyrics'], row['URL'], row['Title'])\n",
    "    }\n",
    "    data[\"essays\"].append(essay)\n",
    "\n",
    "# At this point, `data` is a Python dictionary that matches the PGJSON structure.\n",
    "# You can convert it to a JSON string using json.dumps:\n",
    "import json\n",
    "json_str = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "# Write the JSON string to a file named pg.json\n",
    "with open('pg.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
